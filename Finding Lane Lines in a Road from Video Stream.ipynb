{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Lane Lines in Road \n",
    "In this project, I used Python and OpenCV to find lane lines in a road video stream. \n",
    "\n",
    "**My approach is as follows**:\n",
    "\n",
    " * Identifying Lane Lines by Color Selection\n",
    " * Gaussian Blur to reduce noise\n",
    " * Region Masking or Region of Interest Selection\n",
    " * Canny Edge Detection\n",
    " * Hough Transform to find Lane Lines\n",
    "\n",
    "I applied all the above techniques to process video clips to find lane lines in them using MoviePy which is a Python module for video editing, which can be used for basic operations (like cuts, concatenations, title insertions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Specification \n",
    "\n",
    "## Lane Finding pipeline\n",
    "\n",
    "| Criteria | Meets Specification |\n",
    "| -------- | ------------------- |\n",
    "| Does the pipeline for the line identification take road images from the video as input and return an annotated video stream as output? | The output video is an annotated version of the input video |\n",
    "|Has a pipeline been implemented that uses the helper functions and / or other code to roughly identify the left and right lanes lines with either line segments or solid lines?  |In a rough sense, the left and right lane lines are accurately annotated through almost all of the video. Annotations can be segmented or solid lines. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We import the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip, ImageClip\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods for the pipeline\n",
    "We create helper functions for the pipeline by wrapping OpenCV Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gray Scaling\n",
    "\n",
    "The images should be converted into gray scaled ones in order to detect shapes (edges) in the images.  This is because the Canny edge detection measures the magnitude of pixel intensity changes or gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gray_scale(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny Edge Detection\n",
    "\n",
    "The Canny edge detector was developed by John F. Canny.  \n",
    "\n",
    "We want to detect edges in order to find straight lines especially lane lines.  For this, \n",
    "\n",
    "-  `cv2.cvtColor` to convert images into gray scale\n",
    "-  `cv2.GaussianBlur` to smooth out rough edges \n",
    "-  `cv2.Canny` to find edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def canny_edge(img, low_threshold, high_threshold):\n",
    "    \"\"\" The Canny edge detector is an edge detection operator that uses a\n",
    "    multi-stage algorithm to detect a wide range of edges in images. \"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Blur\n",
    "\n",
    "When there is an edge (i.e. a line), the pixel intensity changes rapidly (i.e. from 0 to 255) The GaussianBlur takes a kernel_size parameter which you'll need to play with to find one that works best. I tried 3, 5, 9, 11, 15, 17 (they must be positive and odd) and check the edge detection result. The bigger the kernel_size value is, the more blurry the image becomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_blur(img, kernel_size):\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the white and yellow lines are clearly recognizable. We build a filter to select the white and yellow lines. we need to select particular range of each channels (Hue, Saturation and Light). For this,\n",
    "\n",
    "- cv2.inRange to filter the white color and the yellow color seperately.\n",
    "- function returns 255 when the filter conditon is satisfied. Otherwise, it returns 0.\n",
    "- cv2.bitwise_or to combine these two binary masks.\n",
    "    The combined mask returns 255 when either white or yellow color is detected.\n",
    "- cv2.bitwise_and to apply the combined mask onto the original RGB image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finding lane lines, we don't need views that are out of context. We are interested in the areas surrounded by the lines only. This is our region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\" Applies an image mask.Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_lines(img, lines, color=(255, 0, 0), thickness=10):\n",
    "    \"\"\"Iterate over the output \"lines\" and draw lines on the blank\"\"\"\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hough Transform Line Detection\n",
    "\n",
    "I'm using `cv2.HoughLinesP` to detect lines in the edge images.\n",
    "\n",
    "There are several parameters you'll need tweaking:\n",
    "\n",
    "- rho – Distance resolution of the accumulator in pixels.\n",
    "- theta – Angle resolution of the accumulator in radians.\n",
    "- threshold – Accumulator threshold parameter. Only those lines are returned that get enough votes (> `threshold`).\n",
    "- minLineLength – Minimum line length. Line segments shorter than that are rejected.\n",
    "- maxLineGap – Maximum allowed gap between points on the same line to link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros(img.shape, dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    return line_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging and Extrapolating Lines\n",
    "\n",
    "There are multiple lines detected for a lane line.  We should come up with an averaged line for that.\n",
    "\n",
    "Also, some lane lines are only partially recognized.  We should extrapolate the line to cover full lane line length.\n",
    "\n",
    "We want two lane lines: one for the left and the other for the right.  The left lane should have a positive slope, and the right lane should have a negative slope.  Therefore, we'll collect positive slope lines and negative slope lines separately and take averages.\n",
    "\n",
    "**in the image, `y` coordinate is reversed.  The higher `y` value is actually lower in the image.  Therefore, the slope is negative for the left lane, and the slope is positive for the right lane.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_lines(lines):\n",
    "    \"\"\" Takes an array of hough lines and separates them by +/- slope.\n",
    "        The y-axis is inverted in matplotlib, so the calculated positive slopes will be right\n",
    "        lane lines and negative slopes will be left lanes. \"\"\"\n",
    "    right = []\n",
    "    left = []\n",
    "    for x1,y1,x2,y2 in lines[:, 0]:\n",
    "        m = (float(y2) - y1) / (x2 - x1)\n",
    "        if m >= 0: \n",
    "            right.append([x1,y1,x2,y2,m])\n",
    "        else:\n",
    "            left.append([x1,y1,x2,y2,m])\n",
    "    \n",
    "    return right, left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend_point(x1, y1, x2, y2, length):\n",
    "    \"\"\" Takes line endpoints and extroplates new endpoint by a specfic length\"\"\"\n",
    "    line_len = np.sqrt((x1 - x2)**2 + (y1 - y2)**2) \n",
    "    x = x2 + (x2 - x1) / line_len * length\n",
    "    y = y2 + (y2 - y1) / line_len * length\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reject_outliers(data, cutoff, thresh=0.08):\n",
    "    \"\"\"Reduces jitter by rejecting lines based on a hard cutoff range and outlier slope \"\"\"\n",
    "    data = np.array(data)\n",
    "    data = data[(data[:, 4] >= cutoff[0]) & (data[:, 4] <= cutoff[1])]\n",
    "    m = np.mean(data[:, 4], axis=0)\n",
    "    return data[(data[:, 4] <= m+thresh) & (data[:, 4] >= m-thresh)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_lines(lines):\n",
    "    \"\"\"Merges all Hough lines by the mean of each endpoint, \n",
    "       then extends them off across the image\"\"\"\n",
    "    \n",
    "    lines = np.array(lines)[:, :4] ## Drop last column (slope)\n",
    "    \n",
    "    x1,y1,x2,y2 = np.mean(lines, axis=0)\n",
    "    x1e, y1e = extend_point(x1,y1,x2,y2, -1000) # bottom point\n",
    "    x2e, y2e = extend_point(x1,y1,x2,y2, 1000)  # top point\n",
    "    line = np.array([[x1e,y1e,x2e,y2e]])\n",
    "    \n",
    "    return np.array([line], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabilizing for shaded areas\n",
    "We create a color mask that will highlights the whites and yellows in the frame. This will ensure the Hough lines are more easily detected in shaded regions or low contrast regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_prev(line, prev):\n",
    "    \"\"\" Extra Challenge: Reduces jitter and missed lines by averaging previous \n",
    "        frame line with current frame line. \"\"\"\n",
    "    if prev != None:\n",
    "        line = np.concatenate((line[0], prev[0]))\n",
    "        x1,y1,x2,y2 = np.mean(line, axis=0)\n",
    "        line = np.array([[[x1,y1,x2,y2]]], dtype=np.int32)\n",
    "        return line\n",
    "    else: \n",
    "        return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipeline \n",
    "We add a global variable for the line from the prior frame. This will be averaged with the current frame to  prevent jittery line detection on the video footage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global right_prev\n",
    "global left_prev\n",
    "right_prev = None\n",
    "left_prev = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline(image, preview=False):\n",
    "    global right_prev\n",
    "    global left_prev\n",
    "    bot_left = [250, 660]\n",
    "    bot_right = [1100, 660]\n",
    "    apex_right = [725, 440]\n",
    "    apex_left = [580, 440]\n",
    "    v = [np.array([bot_left, bot_right, apex_right, apex_left], dtype=np.int32)]\n",
    "    \n",
    "    ### Added a color mask to deal with shaded region\n",
    "    color_low = np.array([187,187,0])\n",
    "    color_high = np.array([255,255,255])\n",
    "    color_mask = cv2.inRange(image, color_low, color_high)\n",
    "    \n",
    "    gray = gray_scale(image)\n",
    "    blur = gaussian_blur(gray, 3)\n",
    "    blur = weighted_img(blur, color_mask)\n",
    "\n",
    "    edge = canny_edge(blur, 100, 300)\n",
    "    mask = region_of_interest(edge, v)\n",
    "\n",
    "    lines = cv2.HoughLinesP(mask, 0.5, np.pi/180, 10, np.array([]), minLineLength=90, maxLineGap=200)\n",
    "\n",
    "    right_lines, left_lines = separate_lines(lines)\n",
    "\n",
    "    right = reject_outliers(right_lines, cutoff=(0.45, 0.75))\n",
    "    right = merge_lines(right)\n",
    "    right = merge_prev(right, right_prev)\n",
    "    right_prev = right\n",
    "\n",
    "    left = reject_outliers(left_lines, cutoff=(-1.1, -0.2))\n",
    "    left = merge_lines(left)\n",
    "    left = merge_prev(left, left_prev)\n",
    "    left_prev = left\n",
    "    \n",
    "    lines = np.concatenate((right, left))\n",
    "    line_img = np.copy((image)*0)\n",
    "    draw_lines(line_img, lines, thickness=10)\n",
    "    \n",
    "    line_img = region_of_interest(line_img, v)\n",
    "    final = weighted_img(line_img, image)\n",
    "    \n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the image pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    result = pipeline(image)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video detectlanes.mp4\n",
      "[MoviePy] Writing video detectlanes.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:18<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: detectlanes.mp4 \n",
      "\n",
      "CPU times: user 4min 23s, sys: 8.19 s, total: 4min 32s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "new_clip_output = 'detectlanes.mp4'\n",
    "clip1 = VideoFileClip(\"test.mp4\")\n",
    "new_clip = clip1.fl_image(process_image)\n",
    "%time new_clip.write_videofile(new_clip_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playback the annotated video with lane detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"300\" controls>\n",
       "  <source src=\"detectlanes.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"300\" controls>\n",
    "  <source src=\"{0}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(new_clip_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using these techniques lane detection works well but does not factor for curvture of the lanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
